{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20기 AI/Bigdata 아카데미 자연어처리 과목의 실습을 위한 견본 코드입니다.\n",
    "\n",
    "Kaggle 링크: https://www.kaggle.com/t/beb77bf068f64c44a1fe2bccabb9ca47\n",
    "\n",
    "하단의 코드 블럭을 실행하여 데이터셋을 불러옵니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T07:15:50.920922Z",
     "start_time": "2022-12-06T07:15:43.936003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\user\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\lib\\site-packages (4.25.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (2022.2.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# 하단의 코드에서 주석(#)을 해제하고, Colab 인스턴스에 transformers 와 datasets 라이브러리를 설치합니다. PyTorch는 Colab 인스턴스에 이미 설치되어 있습니다.\n",
    "! pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T07:16:09.480562Z",
     "start_time": "2022-12-06T07:15:54.104992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25.1\n"
     ]
    }
   ],
   "source": [
    "# 설치된 Transformers lib의 버전이 최소 4.11.0 이상인지 확인합니다.\n",
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T07:16:14.022270Z",
     "start_time": "2022-12-06T07:16:10.631171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# CUDA GPU가 사용 가능한지 확인합니다.\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T07:16:37.586353Z",
     "start_time": "2022-12-06T07:16:19.194407Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc744d188da4007a433dae00b0dd16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/28.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8051d2b983d2447a9719f839e008f3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/28.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28df577ae907434aa83713bbfa274694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/sst2 to C:/Users/user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47becaec56547c4b2d34d49078d7f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.44M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to C:/Users/user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/user/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "# Huggingface Dataset 레포지토리에서 KLUE-Ynat 데이터를 로드합니다.\n",
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset(\"glue\", \"sst2\", split=\"train\")\n",
    "val_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T07:16:37.617141Z",
     "start_time": "2022-12-06T07:16:37.592349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67349\n",
      "872\n"
     ]
    }
   ],
   "source": [
    "# train_dataset과 val_dataset의 크기를 확인합니다.\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T07:16:37.632684Z",
     "start_time": "2022-12-06T07:16:37.619719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0}\n",
      "{'sentence': \"it 's a charming and often affecting journey . \", 'label': 1, 'idx': 0}\n"
     ]
    }
   ],
   "source": [
    "# train_dataset과 val_dataset의 첫 번째 샘플을 출력합니다.\n",
    "print(train_dataset[0])\n",
    "print(val_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casual Language Modeling 학습을 위해 일정 길이로 나눠진 Text Chunk (텍스트 조각)이 필요합니다. 이를 위해서, 데이터셋의 모든 Text를 Tokenize 한 후 하나로 합칩니다.\n",
    "전부 다 이어붙인 Text를 미리 정해진 길이로 나눕니다. 편집 완료된 데이터의 예시는 아래와 같습니다:\n",
    "```\n",
    "part of text 1\n",
    "```\n",
    "or\n",
    "```\n",
    "end of text 1 [BOS_TOKEN] beginning of text 2\n",
    "```\n",
    "\n",
    "텍스트 데이터 별로 학습시키는 것도 가능하지만, 여기서는 batch 관리를 편하게 하기 위해 일렬로 나열한 후 일정 길이 마다 잘라서 사용합니다.\n",
    "\n",
    "두 경우 모두 각 Token에 대해 다음 순번의 Token을 예측하는 것을 Objective Function으로 설정합니다.\n",
    "\n",
    "예시 코드에서는 '[bert-base-uncased](https://huggingface.co/bert-base-uncased)' 모델을 사용합니다.\n",
    "[Repository](https://huggingface.co/models?filter=causal-lm)에서 다양한 모델을 찾을 수 있습니다. 적절한 모델을 찾아 시도해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T07:16:41.826653Z",
     "start_time": "2022-12-06T07:16:41.813651Z"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer와 model을 초기화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T07:17:15.879765Z",
     "start_time": "2022-12-06T07:16:42.729016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d5ef9bc2314284abf8071feb961ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72a4c13613d485dbf5714965779eef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ecf02fa46b41c5b0347075b4dee043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c3e1edbcdb4604b878c78e59a9c5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e003ea539a4fe689b64046bdbb4a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 불러온 모든 Text에 대해 Tokenizer를 적용할 수 있습니다. Huggingface Datasets 라이브러리의 [`map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) 메서드를 통해 데이터에 일괄적으로 전처리 함수를 적용할 수 있습니다.\n",
    "\n",
    "여기서는 tokenizer로 입력된 Text를 Tokenize하는 전처리 함수를 정의합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T07:17:20.609270Z",
     "start_time": "2022-12-06T07:17:20.594310Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    new_data = [s.replace('.','') for s in examples['sentence']]\n",
    "    return tokenizer(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고, 해당 전처리 함수를 `datasets` object의 모든 데이터에 적용합니다. `batched=True`로 설정하고 process를 4로 설정함으로써 처리 속도를 향상시킬 수 있습니다.\n",
    "\n",
    "이 과정에서 에러가 발생할 경우 process 수를 줄이거나 `batched=False`로 설정해서 다시 해보세요.\n",
    "\n",
    "Tokenize 이후에 제출 데이터 제작을 위한 guid를 제외한 텍스트는 필요하지 않으므로, `remove_columns=['sentence','idx','label']`옵션을 통해 해당 Column을 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T07:17:26.372969Z",
     "start_time": "2022-12-06T07:17:22.868312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4779fa4f7f25452d945fdc2fdddcd037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576ff165d0404abe96c5f5fffe1b653b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "872"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=['sentence','idx','label'])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=['sentence','idx','label'])\n",
    "val_dataset_token_count_list = [len(t['input_ids']) for t in tokenized_val_dataset]\n",
    "len(val_dataset_token_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T07:17:27.180908Z",
     "start_time": "2022-12-06T07:17:27.164949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T07:17:27.721182Z",
     "start_time": "2022-12-06T07:17:27.704184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  1037,\n",
       "  11951,\n",
       "  1998,\n",
       "  2411,\n",
       "  12473,\n",
       "  4990,\n",
       "  102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_val_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize를 끝냈으면, 텍스트를 모두 하나로 이어 이를 `block_size` 만큼의 조각으로 나눕니다. 이를 위해 `batched=True`를 활성화 한 `map` method를 한번 더 활용합니다.\n",
    "\n",
    "`batched=True` 옵션은 입력과 출력 데이터의 개수를 다르게 지정할 수 있습니다. 이를 통해 새로운 batch dataset을 생성할 수도 있습니다.\n",
    "\n",
    "`block_size`를 `tokenizer.model_max_length`로 설정하는 것이 일반적이나(BERT의 경우 512), GPU 자원의 한계로 인해 이를 모두 활용하지 못할 수도 있습니다. (Colab 노트북에서는 `tokenizer.model_max_length`로 설정하면 메모리가 부족할 가능성이 매우 높습니다.)\n",
    "\n",
    "GPU 메모리 이슈가 발생할 경우, `block_size`를 128이나 더 작은 값으로 설정하고 다시 해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T07:17:28.814236Z",
     "start_time": "2022-12-06T07:17:28.796275Z"
    }
   },
   "outputs": [],
   "source": [
    "#block_size = tokenizer.model_max_length\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch 생성 함수를 아래와 같이 작성합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T07:17:29.682943Z",
     "start_time": "2022-12-06T07:17:29.671878Z"
    }
   },
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples['input_ids'])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`map` 메서드의 `batch_size` 파라미터의 기본값은 1,000입니다. 즉 1,000 데이터마다 정해진 `block_size`에 맞지 않는 조그마한 데이터가 버려집니다.\n",
    "필요에 따라 `batch_size`를 변경하는 것이 가능합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-06T07:17:30.521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6e55e756c04f16af2264f93d31be59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_train_dataset = tokenized_train_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_val_dataset = tokenized_val_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HyperParameter를 초기화 합니다. 여기서 값을 다양하게 바꾸거나, 다양한 세팅을 시도해 보세요.\n",
    "하단의 코드 블럭에 명시된 파라미터를 컨트롤 하여 실험하는 것으로도 충분하긴 하나, 다른 파라미터를 도입하는 것도 가능합니다.\n",
    "\n",
    "가능한 파라미터의 목록은 [API 문서](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) 를 참조하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-gluesst2\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-6,\n",
    "    weight_decay=0.1,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_train_dataset,\n",
    "    eval_dataset=lm_val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 끝난 Language Model을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle System에 제출할 파일을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "label = []\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    dataset_count = len(tokenized_val_dataset)\n",
    "    sentence_count = 0\n",
    "    idx = 1\n",
    "    predicted_cache = []\n",
    "    for datum in tqdm(tokenized_val_dataset):\n",
    "        input_vec = torch.Tensor(datum['input_ids']).to('cuda',dtype=torch.long).unsqueeze(0)\n",
    "        output = model(input_ids=input_vec)\n",
    "        logits = output.logits[0]\n",
    "        len_vec = len(input_vec[0])\n",
    "        for i in range(len_vec-1):\n",
    "            l = torch.softmax(logits[i],dim=-1)[datum['input_ids'][i+1]].item()\n",
    "            predicted_cache.append(l)\n",
    "        if idx < dataset_count:\n",
    "            predicted_cache.append(torch.softmax(logits[-1],dim=-1)[tokenized_val_dataset[idx]['input_ids'][0]].item())\n",
    "            idx += 1\n",
    "        predicted.append(sum(predicted_cache)/len(predicted_cache))\n",
    "        label.append(1)\n",
    "\n",
    "        # label.append(tokenizer.decode(datum['input_ids'],skip_special_tokens=True))\n",
    "        # input_vec = torch.Tensor(datum['input_ids'][:-2]).to('cuda',dtype=torch.int32).unsqueeze(0)\n",
    "        # generated = model.generate(input_vec,max_length=len(input_vec[0])+1,\n",
    "        #                            no_repeat_ngram_size=2)\n",
    "        # predicted_word.append(tokenizer.decode(generated[0],skip_special_tokens=True))\n",
    "import numpy as np\n",
    "MSE = np.square(np.subtract(predicted, label)).mean()\n",
    "print(\"MSE:{}\".format(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export generated text to .csv file\n",
    "import pandas\n",
    "\n",
    "prw_df = pandas.DataFrame({\"prob\":predicted})\n",
    "prw_df.to_csv(\"./result.csv\",index_label='index')\n",
    "\n",
    "l_df = pandas.DataFrame({\"prob\":label})\n",
    "l_df.to_csv(\"./gold.csv\",index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "09d9649bff3bd2c3676efb9573c2d6400dae37d212533dd3b534bae6b5085415"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
