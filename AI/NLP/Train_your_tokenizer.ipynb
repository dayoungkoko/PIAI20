{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "Colab에서 이 노트북을 연 경우, 필요한 라이브러리를 미리 설치해야 할 가능성이 높습니다. 아래 코드의 주석 (#)을 해제하고 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:28:07.757056Z",
     "start_time": "2022-12-02T02:27:40.981972Z"
    },
    "id": "MOsHUjgdIrIW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n",
      "Collecting transformers[sentencepiece]\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (1.4.2)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.1.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (21.3)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (2022.2.0)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-10.0.1-cp39-cp39-win_amd64.whl (20.3 MB)\n",
      "Collecting dill<0.3.7\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]) (2022.3.15)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]) (3.6.0)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]) (3.19.1)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: tokenizers, huggingface-hub, dill, xxhash, transformers, sentencepiece, responses, pyarrow, multiprocess, datasets\n",
      "Successfully installed datasets-2.7.1 dill-0.3.6 huggingface-hub-0.11.1 multiprocess-0.70.14 pyarrow-10.0.1 responses-0.18.0 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.25.1 xxhash-3.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFASsisvIrIb"
   },
   "source": [
    "로컬에서 이 노트북을 연 경우 (Jypyter Notebook 등) 최신 버전의 Transformer 설치가 완료된 가상환경을 사용해야 합니다. \n",
    "머신러닝딥러닝 과목 실습 시간에 사용한 가상환경을 베이스로 위 코드를 통해 transformer / transformer dataset을 설치해서 사용할 수도 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4LMm2F5p9-H"
   },
   "source": [
    "# Training your own tokenizer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awfhK-BOp9-H"
   },
   "source": [
    "이번 실습을 통해, 주어진 corpus를 활용해서 tokenizer를 fine-tune하는 방법을 실습해봅니다. 이를 활용하여 Language Model을 학습시킬 수 있습니다.\n",
    "\n",
    "Tokenizer를 *학습*하는것이 필요한 이유는, Transformer 모델은 대부분 subword tokenization algorithm을 사용하기 때문입니다. subword tokenization algorithm은 사용하는 corpus에서 자주 등장하는 subword를 분류하도록 tokenizer를 학습 시킵니다.\n",
    "\n",
    "추가적인 읽을거리로 Huggingface [공식 학습 자료](https://huggingface.co/course/chapter2/4?fw=pt)에서 tokenizer에 대한 전반적인 내용을 열람하고, [tokenizers summary](https://huggingface.co/transformers/tokenizer_summary.html) 챕터에서 다양한 subword tokenization algorithm을 접할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:47:53.287754Z",
     "start_time": "2022-12-02T02:47:49.905514Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch-gpu (from versions: none)\n",
      "ERROR: No matching distribution found for torch-gpu\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:44:08.906941Z",
     "start_time": "2022-12-02T02:44:07.182608Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:44:10.754665Z",
     "start_time": "2022-12-02T02:44:10.743694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWUjGq2wp9-I"
   },
   "source": [
    "## Getting a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vk7GjNW5p9-I"
   },
   "source": [
    "tokenizer 학습을 위해 text 데이터가 필요합니다. Huggingface [Datasets](https://github.com/huggingface/datasets) 라이브러리의 `load_dataset` 함수를 활용해서 데이터를 다운로드 받을 수 있습니다. 이는 다음 실습시간과 AI 설계 과정에서도 유용하게 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:28:15.850365Z",
     "start_time": "2022-12-02T02:28:12.413985Z"
    },
    "id": "uvm77RZcp9-J"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9JVJRtHp9-J"
   },
   "source": [
    "이번 실습시간에서는 Wikitext-2 dataset 을 사용해서 tokenizer를 학습합니다. 빠른 실습 진행을 위해 4.5MB 정도로 작은 데이터를 사용했지만 필요에 따라 훨씬 큰 (수백GB 정도의) 데이터를 사용할 수 있고, 영어 뿐만 아니라 다양한 언어로 된 텍스트를 사용할 수도 있습니다. 여러 언어가 섞인 데이터를 사용할 수도 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:28:29.742354Z",
     "start_time": "2022-12-02T02:28:17.844962Z"
    },
    "id": "i2EFmQU8p9-K"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bc38095f54455ca313e4a9364562a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a463487b937c49b0b2b65f4632bff7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/6.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78bca852ec674c2f88006ff449877d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikitext/wikitext-2-raw-v1 to C:/Users/user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83bc161f8a34f3bbb093b87ce3630fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikitext downloaded and prepared to C:/Users/user/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-_EfJxmp9-L"
   },
   "source": [
    "We can have a look at the dataset, which as 36,718 texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:28:32.657443Z",
     "start_time": "2022-12-02T02:28:32.628522Z"
    },
    "id": "NVLpVKm1p9-L"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcmSAnPmp9-M"
   },
   "source": [
    "배열의 원소에 접근하듯이 단일 데이터에 접근할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:28:35.008288Z",
     "start_time": "2022-12-02T02:28:34.996316Z"
    },
    "id": "5Jw6OM6Mp9-M"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' = Valkyria Chronicles III = \\n'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNELVNWUp9-M"
   },
   "source": [
    "배열의 부분배열을 호출하듯이, dataset의 일부를 간단히 접근할 수 있습니다. 아래 코드를 실행하면 `\"text\"` 가 key이고, 5개의 문장 list를 value로 가지고 있는 dictionary를 열람할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:28:36.288560Z",
     "start_time": "2022-12-02T02:28:36.266623Z"
    },
    "id": "L45LoXLEp9-N"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['',\n",
       "  ' = Valkyria Chronicles III = \\n',\n",
       "  '',\n",
       "  ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n',\n",
       "  \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11LgtX7jp9-N"
   },
   "source": [
    "Tokenizer를 학습시키기 위한 Huggingface API는 batch text를 출력하는 iterator를 요구합니다. 예시로 아래와 같이 텍스트 list의 list를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:28:37.188546Z",
     "start_time": "2022-12-02T02:28:37.086705Z"
    },
    "id": "bozakJMOp9-N"
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "all_texts = [dataset[i : i + batch_size][\"text\"] for i in range(0, len(dataset), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYw5W2nap9-N"
   },
   "source": [
    "모든 데이터를 한번에 메모리에 올리는 것을 방지하기 위해 Python Iterator를 정의하여 필요한 데이터만 메모리에 올리도록 합니다. 이는 거대한 데이터셋을 사용하여 학습할 때 유용하게 쓸 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:28:38.019535Z",
     "start_time": "2022-12-02T02:28:38.008528Z"
    },
    "id": "K9fTgRkbp9-O"
   },
   "outputs": [],
   "source": [
    "def batch_iterator():\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVpySxi8p9-O"
   },
   "source": [
    "## Using an existing tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9n4xez7p9-O"
   },
   "source": [
    "기존에 존재하는 tokenizer와 동일한 알고리즘과 파라미터를 사용하여 tokenizer를 학습하고자 한다면, `train_new_from_iterator` API를 사용할 수 있습니다. 예시로, BERT Tokenizer를 wikitext-2 데이터셋을 통해 학습시켜봅시다.\n",
    "\n",
    "먼저, 사용하고자 하는 tokenizer를 load합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:02.344749Z",
     "start_time": "2022-12-02T02:28:38.883311Z"
    },
    "id": "n-Hzcz_np9-P"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d141cd62bca74c6db5d7b9f63341c914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5962934dea964999a32933f88d25c889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c31b9e86ede495c87c4ba15a0c32d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1f2ecdae564735819291c1a67fd140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmNWnVqvp9-P"
   },
   "source": [
    "Load된 tokenizer가 *fast* 버전 (Huggingface backbone)인지 확인합니다. 아니라면 하단의 코드가 작동하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:04.555824Z",
     "start_time": "2022-12-02T02:29:04.538695Z"
    },
    "id": "5rYjlgcop9-P"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4iaaNdLp9-P"
   },
   "source": [
    "이제 앞서 정의한 traning corpus (list의 list 혹은 앞서 정의한 iterator)를 `train_new_from_iterator` 메서드에 입력합니다. 이때 tokenizer가 가질 vocabulary size를 정할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:11.423087Z",
     "start_time": "2022-12-02T02:29:07.315711Z"
    },
    "id": "fdrfE0Ocp9-P"
   },
   "outputs": [],
   "source": [
    "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tEe0D1Up9-P"
   },
   "source": [
    "Tokenizer 학습이 완료되었습니다! 아래와 같이 테스트를 해볼 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:12.663848Z",
     "start_time": "2022-12-02T02:29:12.646897Z"
    },
    "id": "h99u6kaFp9-P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Original Text: = Valkyria Chronicles III = \n",
      "\n",
      "Original Text:\n",
      "Original Text: Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
      "\n",
      "Original Text: The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
      "\n",
      "Tokenized:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 3], [2, 33, 9773, 10627, 4171, 33, 3], [2, 3], [2, 5693, 1067, 1092, 2045, 9773, 23, 30, 1843, 7499, 18065, 1025, 10627, 12, 3923, 30, 810, 758, 617, 1289, 1290, 1129, 1291, 1292, 1293, 1294, 1040, 16, 7591, 18, 9773, 1506, 1495, 4737, 3191, 23, 13, 16, 6295, 4419, 1515, 1546, 9773, 10627, 4171, 3739, 4353, 16, 1559, 69, 15148, 2714, 36, 17, 36, 3527, 2677, 1898, 3106, 1565, 14932, 1015, 1513, 8708, 18, 20509, 1540, 1495, 5956, 16675, 18, 20918, 1509, 2555, 2846, 1509, 4353, 16, 1620, 1559, 1495, 2588, 1898, 1509, 1495, 9773, 2061, 18, 3729, 3012, 1511, 1495, 2271, 10783, 1506, 15148, 1513, 2776, 36, 17, 36, 1814, 7707, 1546, 1699, 15512, 16, 1495, 2642, 4551, 8086, 1515, 1495, 1697, 1898, 1513, 5862, 1495, 6, 23073, 6, 16, 69, 22752, 2893, 4963, 6783, 1495, 5420, 1506, 5831, 1668, 1890, 1495, 4699, 12085, 1019, 2273, 1772, 2117, 5133, 3134, 3697, 1513, 1636, 3275, 1881, 2100, 1495, 8786, 4963, 6, 3171, 3481, 2274, 23306, 6, 18, 3], [2, 1535, 1898, 2183, 3130, 1509, 2681, 16, 8795, 1788, 69, 2401, 5929, 1506, 1495, 1852, 4774, 1536, 9773, 10627, 2402, 18, 3044, 1620, 8761, 1495, 4804, 3441, 1506, 1495, 2061, 16, 1620, 1708, 10766, 6486, 17730, 2233, 16, 1994, 1546, 3032, 1495, 1898, 1830, 11541, 2479, 1540, 2061, 1891, 7108, 1011, 1013, 18, 14453, 9951, 9394, 3725, 6899, 9316, 1035, 1513, 8641, 11195, 18050, 14596, 22420, 4259, 2075, 3048, 1587, 2600, 23729, 16, 2230, 1557, 9773, 10627, 2402, 3642, 22230, 6686, 20484, 6361, 18, 37, 2401, 2151, 1506, 4966, 9841, 1495, 5035, 18, 1535, 1898, 11, 87, 3710, 4651, 1533, 9492, 1565, 2235, 11, 82, 18, 3]], 'token_type_ids': [[0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1], [1, 1, 1, 1, 1, 1, 1], [1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  print(\"Original Text:{}\".format(dataset[i][\"text\"]))\n",
    "print(\"Tokenized:\")\n",
    "new_tokenizer(dataset[:5][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFy9eVmap9-Q"
   },
   "source": [
    "학습이 완료된 tokenizer를 `save_pretrained` 메서드로 로컬로 저장할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:13.709772Z",
     "start_time": "2022-12-02T02:29:13.643715Z"
    },
    "id": "mjz3T5TDp9-Q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my-new-tokenizer\\\\tokenizer_config.json',\n",
       " 'my-new-tokenizer\\\\special_tokens_map.json',\n",
       " 'my-new-tokenizer\\\\vocab.txt',\n",
       " 'my-new-tokenizer\\\\added_tokens.json',\n",
       " 'my-new-tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.save_pretrained(\"my-new-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WhNtqoKIJBx"
   },
   "source": [
    "아래 블럭은 Huggingface 에서 제공하는 tokenizer 관련 튜토리얼 중, 제로 베이스에서 tokenizer를 학습 시키는 방법에 대한 내용입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPpqkTSwp9-R"
   },
   "source": [
    "## Building your tokenizer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9Q0rV3Bp9-R"
   },
   "source": [
    "To understand how to build your tokenizer from scratch, we have to dive a little bit more in the 🤗 Tokenizers library and the tokenization pipeline. This pipeline takes several steps:\n",
    "\n",
    "- **Normalization**: Executes all the initial transformations over the initial input string. For example when you need to lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer.\n",
    "- **Pre-tokenization**: In charge of splitting the initial input string. That's the component that decides where and how to pre-segment the origin string. The simplest example would be to simply split on spaces.\n",
    "- **Model**: Handles all the sub-token discovery and generation, this is the part that is trainable and really dependent of your input data.\n",
    "- **Post-Processing**: Provides advanced construction features to be compatible with some of the Transformers-based SoTA models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens.\n",
    "\n",
    "And to go in the other direction:\n",
    "\n",
    "- **Decoding**: In charge of mapping back a tokenized input to the original string. The decoder is usually chosen according to the `PreTokenizer` we used previously.\n",
    "\n",
    "For the training of the model, the 🤗 Tokenizers library provides a `Trainer` class that we will use.\n",
    "\n",
    "All of these building blocks can be combined to create working tokenization pipelines. To give you some examples, we will show three full pipelines here: how to replicate GPT-2, BERT and T5 (which will give you an example of BPE, WordPiece and Unigram tokenizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjoz7fI-p9-R"
   },
   "source": [
    "### WordPiece model like BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcRLcogfp9-R"
   },
   "source": [
    "Let's have a look at how we can create a WordPiece tokenizer like the one used for training BERT. The first step is to create a `Tokenizer` with an empty `WordPiece` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:15.527762Z",
     "start_time": "2022-12-02T02:29:15.503313Z"
    },
    "id": "ec0535I8p9-S"
   },
   "outputs": [],
   "source": [
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unl_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Au5BXQI2p9-S"
   },
   "source": [
    "This `tokenizer` is not ready for training yet. We have to add some preprocessing steps: the normalization (which is optional) and the pre-tokenizer, which will split inputs into the chunks we will call words. The tokens will then be part of those words (but can't be larger than that).\n",
    "\n",
    "In the case of BERT, the normalization is lowercasing. Since BERT is such a popular model, it has its own normalizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:16.348329Z",
     "start_time": "2022-12-02T02:29:16.334198Z"
    },
    "id": "ORgVr_Cap9-S"
   },
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhbNtLc8p9-S"
   },
   "source": [
    "If you want to customize it, you can use the existing blocks and compose them in a sequence: here for instance we lower case, apply NFD normalization and strip the accents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:16.985052Z",
     "start_time": "2022-12-02T02:29:16.980067Z"
    },
    "id": "YbuXiWCUp9-S"
   },
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGDL6Zicp9-S"
   },
   "source": [
    "There is also a `BertPreTokenizer` we can use directly. It pre-tokenizes using white space and punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:17.974266Z",
     "start_time": "2022-12-02T02:29:17.963287Z"
    },
    "id": "lgHwxugPp9-S"
   },
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcvcydpUp9-S"
   },
   "source": [
    "Like for the normalizer, we can combine several pre-tokenizers in a `Sequence`. If we want to have a quick look at how it preprocesses the inputs, we can call the `pre_tokenize_str` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:18.542405Z",
     "start_time": "2022-12-02T02:29:18.526447Z"
    },
    "id": "w8f_2Fbnp9-S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', (0, 4)),\n",
       " ('is', (5, 7)),\n",
       " ('an', (8, 10)),\n",
       " ('example', (11, 18)),\n",
       " ('!', (18, 19))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVhklwz4p9-T"
   },
   "source": [
    "Note that the pre-tokenizer not only split the text into words but keeps the offsets, that is the beginning and start of each of those words inside the original text. This is what will allow the final tokenizer to be able to match each token to the part of the text that it comes from (a feature we use for question answering or token classification tasks).\n",
    "\n",
    "We can now train our tokenizer (the pipeline is not entirely finished but we will need a trained tokenizer to build the post-processor), we use a `WordPieceTrainer` for that. The key thing to remember is to pass along the special tokens to the trainer, as they won't be seen in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:19.126893Z",
     "start_time": "2022-12-02T02:29:19.106834Z"
    },
    "id": "kjTfqtg2p9-T"
   },
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8TJCnzPp9-T"
   },
   "source": [
    "To actually train the tokenizer, the method looks like what we used before: we can either pass some text files, or an iterator of batches of texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:23.368787Z",
     "start_time": "2022-12-02T02:29:19.627955Z"
    },
    "id": "MUs9qJNKp9-T"
   },
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kFcgUeQp9-T"
   },
   "source": [
    "Now that the tokenizer is trained, we can define the post-processor: we need to add the CLS token at the beginning and the SEP token at the end (for single sentences) or several SEP tokens (for pairs of sentences). We use a [`TemplateProcessing`](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizers.processors.TemplateProcessing) to do this, which requires to know the IDs of the CLS and SEP token (which is why we waited for the training).\n",
    "\n",
    "So let's first grab the ids of the two special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:24.146741Z",
     "start_time": "2022-12-02T02:29:24.138763Z"
    },
    "id": "_oM4ctxJp9-T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZIlQxjHp9-T"
   },
   "source": [
    "And here is how we can build our post processor. We have to indicate in the template how to organize the special tokens with one sentence (`$A`) or two sentences (`$A` and `$B`). The `:` followed by a number indicates the token type ID to give to each part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:25.003930Z",
     "start_time": "2022-12-02T02:29:24.985981Z"
    },
    "id": "nWAA6gE6p9-T"
   },
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", cls_token_id),\n",
    "        (\"[SEP]\", sep_token_id),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfGNOKYqp9-T"
   },
   "source": [
    "We can check we get the expected results by encoding a pair of sentences for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:25.660800Z",
     "start_time": "2022-12-02T02:29:25.647793Z"
    },
    "id": "GoyGNV2fp9-U"
   },
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"This is one sentence.\", \"With this one we have a pair.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFzv8t_Fp9-U"
   },
   "source": [
    "We can look at the tokens to check the special tokens have been inserted in the right places:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:26.291759Z",
     "start_time": "2022-12-02T02:29:26.281749Z"
    },
    "id": "0AxmUhMBp9-U"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'one',\n",
       " 'sentence',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'with',\n",
       " 'this',\n",
       " 'one',\n",
       " 'we',\n",
       " 'have',\n",
       " 'a',\n",
       " 'pair',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7dgSQfwp9-U"
   },
   "source": [
    "And we can check the token type ids are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:27.089348Z",
     "start_time": "2022-12-02T02:29:27.068407Z"
    },
    "id": "Fn9zO04dp9-U"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCr3PsUrp9-U"
   },
   "source": [
    "The last piece in this tokenizer is the decoder, we use a `WordPiece` decoder and indicate the special prefix `##`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:27.750147Z",
     "start_time": "2022-12-02T02:29:27.742128Z"
    },
    "id": "k58i7OJvp9-U"
   },
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLSVxUDhp9-U"
   },
   "source": [
    "Now that our tokenizer is finished, we need to wrap it inside a Transformers object to be able to use it with the Transformers library. More specifically, we have to put it inside the class of tokenizer fast corresponding to the model we want to use, here a `BertTokenizerFast`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:28.427334Z",
     "start_time": "2022-12-02T02:29:28.381985Z"
    },
    "id": "NXZXWKurp9-U"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "new_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cX4SfHzp9-U"
   },
   "source": [
    "And like before, we can use this tokenizer as a normal Transformers tokenizer, and use the `save_pretrained` or `push_to_hub` methods.\n",
    "\n",
    "If the tokenizer you are building does not match any class in Transformers because it's really special, you can wrap it in `PreTrainedTokenizerFast`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeG516Ipp9-U"
   },
   "source": [
    "### BPE model like GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ha4wzxDBp9-U"
   },
   "source": [
    "Let's now have a look at how we can create a BPE tokenizer like the one used for training GPT-2. The first step is to create a `Tokenizer` with an empty `BPE` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:29.840045Z",
     "start_time": "2022-12-02T02:29:29.822093Z"
    },
    "id": "GA0euRmrp9-V"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3SMiw9op9-V"
   },
   "source": [
    "Like before, we have to add the optional normalization (not used in the case of GPT-2) and we need to specify a pre-tokenizer before training. In the case of GPT-2, the pre-tokenizer used is a byte level pre-tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:30.499854Z",
     "start_time": "2022-12-02T02:29:30.490870Z"
    },
    "id": "QL99BWibp9-V"
   },
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnrayq3Gp9-V"
   },
   "source": [
    "If we want to have a quick look at how it preprocesses the inputs, we can call the `pre_tokenize_str` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:31.198840Z",
     "start_time": "2022-12-02T02:29:31.183296Z"
    },
    "id": "96XSUWQHp9-V"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', (0, 4)),\n",
       " ('Ġis', (4, 7)),\n",
       " ('Ġan', (7, 10)),\n",
       " ('Ġexample', (10, 18)),\n",
       " ('!', (18, 19))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3txhLA-p9-V"
   },
   "source": [
    "We used the same default as for GPT-2 for the prefix space, so you can see that each word gets an initial `'Ġ'` added at the beginning, except the first one.\n",
    "\n",
    "We can now train our tokenizer! This time we use a `BpeTrainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:36.151608Z",
     "start_time": "2022-12-02T02:29:31.935909Z"
    },
    "id": "LVxiHcD4p9-V"
   },
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV5YiiCWp9-V"
   },
   "source": [
    "To finish the whole pipeline, we have to include the post-processor and decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:36.877655Z",
     "start_time": "2022-12-02T02:29:36.865648Z"
    },
    "id": "nJaSPLbKp9-V"
   },
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdg-J21ep9-V"
   },
   "source": [
    "And like before, we finish by wrapping this in a Transformers tokenizer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:37.809817Z",
     "start_time": "2022-12-02T02:29:37.662653Z"
    },
    "id": "qCzrWnr3p9-V"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "new_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KydxMcMhp9-V"
   },
   "source": [
    "### Unigram model like Albert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ypk5umd6p9-V"
   },
   "source": [
    "Let's now have a look at how we can create a Unigram tokenizer like the one used for training T5. The first step is to create a `Tokenizer` with an empty `Unigram` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:38.698470Z",
     "start_time": "2022-12-02T02:29:38.673536Z"
    },
    "id": "fnjqVBKYp9-W"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JskGAUXJp9-W"
   },
   "source": [
    "Like before, we have to add the optional normalization (here some replaces and lower-casing) and we need to specify a pre-tokenizer before training. The pre-tokenizer used is a `Metaspace` pre-tokenizer: it replaces all spaces by a special character (defaulting to ▁) and then splits on that character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:39.216442Z",
     "start_time": "2022-12-02T02:29:39.203478Z"
    },
    "id": "fs1N-34op9-W"
   },
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.Replace(\"``\", '\"'), normalizers.Replace(\"''\", '\"'), normalizers.Lowercase()]\n",
    ")\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhU4DvJqp9-W"
   },
   "source": [
    "If we want to have a quick look at how it preprocesses the inputs, we can call the `pre_tokenize_str` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:39.705413Z",
     "start_time": "2022-12-02T02:29:39.693445Z"
    },
    "id": "-gpsKNcHp9-W"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁This', (0, 4)), ('▁is', (4, 7)), ('▁an', (7, 10)), ('▁example!', (10, 19))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRlaf2t1p9-W"
   },
   "source": [
    "You can see that each word gets an initial `▁` added at the beginning, as is usually done by sentencepiece.\n",
    "\n",
    "We can now train our tokenizer! This time we use a `UnigramTrainer`.\"We have to explicitely set the unknown token in this trainer otherwise it will forget it afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:51.806863Z",
     "start_time": "2022-12-02T02:29:40.132104Z"
    },
    "id": "x39GtATxp9-W"
   },
   "outputs": [],
   "source": [
    "trainer = trainers.UnigramTrainer(vocab_size=25000, special_tokens=[\"[CLS]\", \"[SEP]\", \"<unk>\", \"<pad>\", \"[MASK]\"], unk_token=\"<unk>\")\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EPGEKcup9-W"
   },
   "source": [
    "To finish the whole pipeline, we have to include the post-processor and decoder. The post-processor is very similar to what we saw with BERT, the decoder is just `Metaspace`, like for the pre-tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:52.441784Z",
     "start_time": "2022-12-02T02:29:52.424932Z"
    },
    "id": "id9c7g1yp9-W"
   },
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:52.868783Z",
     "start_time": "2022-12-02T02:29:52.860845Z"
    },
    "id": "On6uNXzHp9-W"
   },
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", cls_token_id),\n",
    "        (\"[SEP]\", sep_token_id),\n",
    "    ],\n",
    ")\n",
    "tokenizer.decoder = decoders.Metaspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5c7w9P8p9-W"
   },
   "source": [
    "And like before, we finish by wrapping this in a Transformers tokenizer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T02:29:54.233650Z",
     "start_time": "2022-12-02T02:29:54.168522Z"
    },
    "id": "5BxpWqGzp9-W"
   },
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizerFast\n",
    "\n",
    "new_tokenizer = AlbertTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4ImTy5rp9-W"
   },
   "source": [
    "## Use your new tokenizer to train a language model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOhn33Mpp9-X"
   },
   "source": [
    "You can either use your new tokenizer in the language modeling from scratch notebook [Link to come] or use the `--tokenizer_name` argument in the [language modeling scripts](https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling) to use it there to train a model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxR2DPhap9-X"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "qjoz7fI-p9-R",
    "TeG516Ipp9-U",
    "KydxMcMhp9-V"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
